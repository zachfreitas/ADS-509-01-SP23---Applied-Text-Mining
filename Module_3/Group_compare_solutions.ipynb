{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Moby Dick by Herman Melville 1851>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These first two functions are modified from the pattern assignrnent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_normal(text): \n",
    "\n",
    "    # Lowercase and split on whitespace \n",
    "    text = text.lower().strip().split() \n",
    "\n",
    "    # Drop nonâ€”alpha and stopwords \n",
    "    text =  [w for w in text if w not in sw and w.isalpha()]\n",
    "\n",
    "    return(text) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patterns(text, num_words):\n",
    "    \"\"\" \n",
    "    This function takes text as an input and returns a dictionary Of statistics, \n",
    "    after cleaning the text. \n",
    "    \"\"\"\n",
    "    if (len(text) == 0):\n",
    "        raise ValueError(\"Can't work with empty text object.\") \n",
    "\n",
    "\n",
    "    # We'll make things a big clearer by the \n",
    "    # statistics here. These are placeholder values. \n",
    "    total_tokens = 1\n",
    "    unique_tokens = 0\n",
    "    avg_token_len = 0.0\n",
    "    lex_diversity = 0.0\n",
    "    top_words =[]\n",
    "    \n",
    "    text = token_normal(text) \n",
    "\n",
    "    if len(text) == 0:\n",
    "        raise ValueError( \" All of text is stopwords! \" ) \n",
    "\n",
    "\n",
    "    # Calculate your statistics here \n",
    "    total_tokens = len(text) \n",
    "    unique_tokens = len(set(text)) \n",
    "    avg_token_len = np.mean([len(w) for w in text]) \n",
    "    lex_diversity = unique_tokens/total_tokens\n",
    "\n",
    "    top_words = FreqDist(text).most_common(num_words) \n",
    "    \n",
    "    # Now we'll fill out the dictionary. \n",
    "    results = { 'tokens' : total_tokens, \n",
    "            'unique_tokens' : unique_tokens,\n",
    "            'avg_token_length' : avg_token_len,\n",
    "            'lexical_diversity': lex_diversity,\n",
    "            'top_words': top_words} \n",
    "\n",
    "    return(results) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are useful for the second part of the assignment. You may have seen these at the end of a class when I was coding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frac(word, fd_corpus, length):  \n",
    "\n",
    "    if word in fd_corpus:\n",
    "        return(fd_corpus[word]/length) \n",
    "    else: \n",
    "        return(0)\n",
    "\n",
    "def get_ratio(word, fd_corpus_1, fd_corpus_2, len_1, len_2):\n",
    "\n",
    "    frac_1 = get_word_frac(word, fd_corpus_1, len_1) \n",
    "    frac_2 = get_word_frac(word, fd_corpus_2, len_2) \n",
    "\n",
    "    if frac_2 > 0:\n",
    "        return(frac_1/frac_2) \n",
    "    else:\n",
    "        return(float('NaN')) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start with the main function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_texts(corpus_1, corpus_2, num_words = 10, ratio_cutoff=5): \n",
    "    \"\"\"\n",
    "    This function returns a nested dictionary with information comparing two groups Of \n",
    "    text. See README for full description of what this function does. \n",
    "    \"\"\"\n",
    "    results = dict() \n",
    "\n",
    "    # Get the first two parts done with a function \n",
    "    results[\"one\"] = get_patterns(corpus_1, num_words)\n",
    "    results[\"two\"] = get_patterns(corpus_2, num_words)\n",
    "\n",
    "    # Now we start the ratio part. Cleaning first, then build \n",
    "    # frequency distributions \n",
    "    corpus_1 = token_normal(corpus_1) \n",
    "    corpus_2 =  token_normal(corpus_2)\n",
    "    \n",
    "    fd_1 = FreqDist(corpus_1) \n",
    "    fd_2 = FreqDist(corpus_2)\n",
    "\n",
    "    # It's handy to have a set of the words in each corpus. \n",
    "    \n",
    "    fd_1_words = set(fd_1.keys()) \n",
    "    fd_2_words = set(fd_2.keys()) \n",
    "    \n",
    "    # This will hold our ratios. Starting with 1 over 2 \n",
    "    holder = dict() \n",
    "    \n",
    "    # Also, we need to tell Python that the \"one_vs two\" spot holds \n",
    "    # a dictionary. (And \"two vs one\") \n",
    "    results[\"one_vs_two\"] = dict() \n",
    "    results[\"two_vs_one\"] = dict() \n",
    "    \n",
    "    # Now we add them. We check along the to make Sure \n",
    "    for word, count in fd_1.items():\n",
    "        if count > ratio_cutoff:\n",
    "            # This next line makes use of the fact that \n",
    "            # Python stops evaluating \"and\" expressions if it hits a False \n",
    "            if word in fd_2_words and fd_2[word] > ratio_cutoff:\n",
    "                holder[word] = get_ratio(word, fd_1, fd_2,\n",
    "                results[\"one\"][\"tokens\"],\n",
    "                results[\"two\"][\"tokens\"])\n",
    "    \n",
    "    num_added = 0\n",
    "\n",
    "    for word, frac in sorted(holder.items() , key=lambda item: -1*item[1]):\n",
    "        results[\"one_vs_two\"][word] = frac\n",
    "        num_added += 1\n",
    "        if num_added == num_words:\n",
    "            break \n",
    "\n",
    "    # Now we do the same for 2 vs 1!\n",
    "    holder = dict() \n",
    "    \n",
    "    # Now we add them. We check along the to make Sure \n",
    "    for word, count in fd_2.items():\n",
    "        if count > ratio_cutoff:\n",
    "            # This next line makes use of the fact that \n",
    "            # Python stops evaluating \"and\" expressions if it hits a False \n",
    "            if word in fd_1_words and fd_1[word] > ratio_cutoff:\n",
    "                holder[word] = get_ratio(word, fd_2, fd_1,\n",
    "                results[\"two\"][\"tokens\"],\n",
    "                results[\"one\"][\"tokens\"])\n",
    "    \n",
    "    num_added = 0    \n",
    "    \n",
    "    for word, frac in sorted(holder.items() , key=lambda item: -1*item[1]):\n",
    "        results[\"two_vs_one\"][word] = frac\n",
    "        num_added += 1\n",
    "        if num_added == num_words:\n",
    "            break \n",
    "    \n",
    "    return(results) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's just test it quickly on textl (Moby Dick) and text2 (Sense and Sensibility) before we finish the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': {'tokens': 110459,\n",
       "  'unique_tokens': 16802,\n",
       "  'avg_token_length': 5.847671986891064,\n",
       "  'lexical_diversity': 0.15211073792085752,\n",
       "  'top_words': [('whale', 1226),\n",
       "   ('one', 921),\n",
       "   ('like', 647),\n",
       "   ('upon', 566),\n",
       "   ('man', 527)]},\n",
       " 'two': {'tokens': 53986,\n",
       "  'unique_tokens': 6148,\n",
       "  'avg_token_length': 6.225817804616011,\n",
       "  'lexical_diversity': 0.11388137665320638,\n",
       "  'top_words': [('elinor', 685),\n",
       "   ('could', 578),\n",
       "   ('marianne', 566),\n",
       "   ('mrs', 530),\n",
       "   ('would', 515)]},\n",
       " 'one_vs_two': {'sort': 8.254316584031683,\n",
       "  'god': 7.428884925628514,\n",
       "  'wind': 5.7019950087060955,\n",
       "  'along': 5.37616672249432,\n",
       "  'boy': 5.294709650941375},\n",
       " 'two_vs_one': {'sister': 96.16517245211722,\n",
       "  'mrs': 83.41659803538809,\n",
       "  'lady': 36.31769810691661,\n",
       "  'mother': 29.326967485397446,\n",
       "  'john': 25.811928448686125}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts ( \" \".join(text1) , \" \".join(text2), num_words=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b33cfd37f5195bd7836c1451c6eaacc84fbbad3c54541ec8bad2790bfb3f777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
